{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from parsing_toolbox import load_sentences_persons\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "EPISODES_LEARN = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "EPISODES_TEST = [9, 10, 11, 12]\n",
    "PERSONS = ['howard_wolowitz', 'sheldon_cooper', 'leonard_hofstadter', 'penny',\n",
    "           'rajesh_koothrappali']\n",
    "GLOVE_DIR = 'data/'\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3506 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "sentences, labels, _, _ = load_sentences_persons(EPISODES_LEARN, states=PERSONS)\n",
    "\n",
    "maxlen = 500  # We will cut sentence after 461 words (max is 461))\n",
    "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
    "\n",
    "# HERE texts are a list of sentences\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2730, 500)\n",
      "Shape of label tensor: (2730,)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# Split the data into a training set and a validation set\n",
    "# But first, shuffle the data, since we started from data\n",
    "# where sample are ordered (all negative first, then all positive).\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "n_classes = len(PERSONS) + 1\n",
    "n_samples = len(data)\n",
    "\n",
    "x_train = data[:round(0.8*n_samples)]\n",
    "y_train = labels[:round(0.8*n_samples)]\n",
    "x_val = data[round(0.8*n_samples):]\n",
    "y_val = labels[round(0.8*n_samples):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Loading pre-embedding data\n",
    "embeddings_index = {}\n",
    "# WARNING watch the embedding dim\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.'+str(EMBEDDING_DIM)+'d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 150000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4800032   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 7,800,230\n",
      "Trainable params: 7,800,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, EMBEDDING_DIM, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(10000, activation='relu'))\n",
    "# model.add(Dense(5000, activation='relu'))\n",
    "# model.add(Dense(1000, activation='relu'))\n",
    "# model.add(Dense(2048, activation='relu'))\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')\n",
    "\n",
    "\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1212, 500) (1212, 6)\n",
      "ACCURACY\n",
      "howard_wolowitz: 0.5556\n",
      "sheldon_cooper: 0.6667\n",
      "leonard_hofstadter: 0.8889\n",
      "penny: 0.6667\n",
      "rajesh_koothrappali: 0.7778\n",
      "unknown: 0.7778\n"
     ]
    }
   ],
   "source": [
    "sentences_test, labels_test, n_ep_test, n_scene_test = load_sentences_persons(EPISODES_TEST)\n",
    "\n",
    "n_ep_test=np.array(n_ep_test).astype(float).astype(int)\n",
    "n_scene_test=np.array(n_scene_test).astype(float).astype(int)\n",
    "\n",
    "sequences_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "x_test = pad_sequences(sequences_test, maxlen=maxlen)\n",
    "y_test = np.asarray(labels_test)\n",
    "y_test = to_categorical(y_test)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "#print('TEST ACCURACY ON SENTENCE:', model.evaluate(x_test, y_test)[1])\n",
    "threshold_prediction = 0.02\n",
    "\n",
    "confusion_per_character = np.zeros((n_classes, 2, 2))\n",
    "for ep in range(len(n_ep_test)):\n",
    "    idx_ep = n_ep_test==ep\n",
    "    ep_scene = n_scene_test[idx_ep]\n",
    "\n",
    "    for sc in list(np.unique(ep_scene)):\n",
    "        #print('({}, {})'.format(ep, sc))\n",
    "        idx_scene = n_scene_test==sc\n",
    "        id_scene = np.logical_and(idx_ep, idx_scene)\n",
    "        # print(id_scene)\n",
    "        x_scene = x_test[id_scene]\n",
    "        y_scene = y_test[id_scene]\n",
    "\n",
    "        predict_scene_by_sentence = np.array(model.predict(x_scene))\n",
    "        predict_scene = np.sum(predict_scene_by_sentence, axis=0)/predict_scene_by_sentence.shape[0]\n",
    "        truth_class = np.unique(np.argmax(y_scene, axis=1))\n",
    "        predict_class = predict_scene[predict_scene>threshold_prediction].argsort()\n",
    "        \n",
    "        for character in range(n_classes):\n",
    "            if character in truth_class and character in predict_class:\n",
    "                confusion_per_character[character, 0, 0] += 1\n",
    "            elif character in truth_class and character not in predict_class:\n",
    "                confusion_per_character[character, 0, 1] += 1\n",
    "            elif character not in truth_class and character in predict_class:\n",
    "                confusion_per_character[character, 1, 0] += 1\n",
    "            elif character not in truth_class and character not in predict_class:\n",
    "                confusion_per_character[character, 1, 1] += 1\n",
    "                \n",
    "PERSONSS = PERSONS + ['unknown']\n",
    "print('ACCURACY')\n",
    "for character in range(n_classes):\n",
    "    m_confusion = confusion_per_character[character, :, :]\n",
    "    #print(m_confusion)\n",
    "    print('{}: {:.4f}'.format(PERSONSS[character], (m_confusion[0,0]+m_confusion[1,1])/m_confusion.sum()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
